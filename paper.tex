% JuliaCon proceedings template
\documentclass{juliacon}
\setcounter{page}{1}
\usepackage{amsmath}

\begin{document}

\input{header}
\maketitle

\begin{abstract}
  Reliable numerical computations are central to HPC and ML.
  We present FlowFPX: a Julia-based tool for tracking the onset and flow of IEEE Floating-Point exceptions that signal numerical defects.
  FlowFPX's design exploits Julia's operator overloading to trace exception flows and even inject exceptions to accelerate testing.
  We present intuitive visualizations of summarized exception flows including how they are generated, propagated and killed, thus helping with debugging and repair.
\end{abstract}

\section{Introduction}

\section{To Kill a Floating Point}
% If anyone has a better "kill" pun/reference than "To Kill a Mockingbird", I'm all ears.

% I think we will definitely need a clear, detailed section on how NaNs crop up and how they can be killed.

The IEEE 754 specification for floating-point numbers \cite{IEEEStandardBinary1985} governs how NaNs are handled as arguments to arithmetic functions.
Most of the time, when any or all of the arguments are NaN, the result is also a NaN.
This is good and desirable, because it means that an arithmetic error---even if buried deep in an expression---will still be observable, allowing a user to find and debug it.

However, there are a few places where NaNs flow into an expression, but not out.
We call these instances ``NaN-kills'', and these can lead to subtle logic bugs.
Consider the following example:

\begin{lstlisting}[language = Julia]
function bad_max(lst)
  max_seen = 0.0
  for x in lst
    if ! (x <= max_seen)
      # swap if new val greater
      max_seen = x
    end
  end
  max_seen
end

function good_max(lst)
  foldl(max, lst)
end
\end{lstlisting}

Both \texttt{bad\_max} and \texttt{good\_max} compute the maximum in a list.
However, the \texttt{<=} operator used in the \texttt{bad\_max} implementation does \emph{not} propagate NaNs, so you might not get the result you expected:

\begin{lstlisting}[language = Julia]
bad_max([1, 5, NaN, 4])     # returns 4
good_max([1, 5, NaN, 4])    # returns NaN
\end{lstlisting}

Not only is the result from \texttt{bad\_max} problematic for obscuring the fact that there was a NaN in the list, the result is also wrong!
The built-in \texttt{max} function correctly propagates NaNs, so we can see from its return that we got a NaN somewhere.
It's not entirely obvious \emph{where} that NaN came from---but more on that later.   % this promise answered in "NaN propagation monitoring"

\subsection{Where do NaNs come from?}

IEEE 754 \cite{IEEEStandardBinary1985} lists the following operations (among a few others) where NaNs result:

\begin{itemize}
\item $\frac{0}{0}$ or $\frac{\infty}{\infty}$
\item $0 * \infty$
\item $x \% y$ when $x = \infty$ or $y = 0$
\item $+\infty + -\infty$ or $\infty - \infty$
\item The square root or logarithm of a negative number
\end{itemize}

% Yadda yadda yadda...

\subsection{NaN kills}

Most mathematical operations preserve NaNs.
However, there are some instances where NaNs can disappear, but not before potentially altering the correctness of the program!

% \begin{itemize}
% \item $x \; \texttt{cmp} \; y \rightarrow false$ where \texttt{cmp} is a binary comparison like \texttt{<}, \texttt{>}, \texttt{<=}, etc. and $x$ or $y$ is NaN.
% \item $1^{NaN} \rightarrow 1$
% \item $NaN^{0} \rightarrow 1$
% \end{itemize}

\begin{align*}
x \; \texttt{cmp} \; y &\rightarrow false &\text{where \texttt{cmp} is \texttt{<}, \texttt{>}, \texttt{<=}, etc.} \\
1^{\text{\texttt{NaN}}} &\rightarrow 1 \\
\text{\texttt{NaN}}^{0} &\rightarrow 1
\end{align*}

% Yadda yadda yadda...

% What else do we need to talk about? What other problems do we need to set up so that FloatTracker's solutions are clear?

\section{FloatTracker Internals}

FloatTracker works by leveraging Julia's type-based method dispatch system to capture everything from calls to the standard library down to basic arithmetic operations.
We do this by creating custom types \texttt{TrackedFloat16}, \texttt{TrackedFloat32}, and \texttt{TrackedFloat64} that wrap their corresponding \texttt{Float16}, \texttt{Float32} etc. counterparts.
We then defined methods for every function in the \texttt{Base} module, including new implementations for operators like \texttt{+} and \texttt{*}.
These new methods had to be defined for every combination of tracked/untracked arguments;
fortunately Julia's metaprogramming capabilities make this relatively simple.

The surrogate methods in place, every operation on a TrackedFloat can be intercepted.
We have three primary goals when intercepting a float:

\begin{enumerate}
  \item Catch instances of NaN kills
  \item Monitor the propagation of NaNs through the program
  \item Optionally \emph{inject} NaNs to fuzz the program under scrutiny against NaN-kills
\end{enumerate}

% Good things to put in this section:
% - start with the max example, perhaps?
% - trace examples
% - CSTGs
%
% Don't forget to talk about trace recording

\subsection{Catching NaN kills}

\subsection{NaN propagation monitoring}

Remember in our first example with the \texttt{bad\_max} and \texttt{good\_max} functions how we didn't have a way to tell \emph{where} the NaNs originally came from?
That can be a tedious task.
However, FloatTracker makes this a lot easier by watching the life cycle of a NaN.

% FIXME: talk about CSTGs

\subsection{NaN injection}

\section{Case Studies}

To illustrate what FloatTracker is capable of, we applied our tool to three different Julia libraries to help us track down strange behavior:

\begin{description}
\item[Surprise NaNs] We found that otherwise innocuous values for a time step parameter in the \texttt{ShallowWaters} library can produce NaNs in the result.
  We used FloatTracker to track down where the NaNs arose.
\item[NaN Fuzzing] We injected NaNs in a N-Body simulation and were able to find a bug in the widely-used \texttt{OrdinaryDiffEq} library.
\item[NaN Kills] We observed issues with NaN kills inside of the \texttt{Finch} library.
  % FIXME: more needed here
\end{description}

All three libraries are available from the JuliaHub package archive.
The \texttt{OrdinaryDiffEq} library in particular enjoys widespread use in the scientific computing community.

\subsection{Surprise NaNs: ShallowWaters}

\subsection{NaN Fuzzing: OrdinaryDiffEq}

The second thing we evaluated was an N-Body simulation library.
We didn't find any sensible configuration of input parameters that lead to NaN kills like in the ShallowWaters library.
We turned to the NaN injection capabilities of FloatTracker to fuzz the library to find any lurking bugs.
% FIXME: add hyper-ref to § 4.1: ShallowWaters

We configured FloatTracker to inject a single NaN to see if we could find any NaN kills.
On some runs we noticed that we would get a kill and the program would warn about a NaN and prepare to exit.
Contrary to the error message's claim, however, the program went into an infinite loop.

The logs lead us to a routine in the widely-used \texttt{OrdinaryDiffEq} library.
During initialization, a NaN got injected during the execution of \texttt{-} between two \texttt{TrackedFloat}s:

\begin{verbatim}
…
- at FloatTracker/src/TrackedFloat.jl:89
#__init#628 at OrdinaryDiffEq/src/solve.jl:106
…
\end{verbatim}

That injection point is here on third line of this snippet, which corresponds to line 106 from the \texttt{solve.jl} file in the \texttt{OrdinaryDiffEq} library:

\begin{lstlisting}[language = Julia]
tType = eltype(prob.tspan)
tspan = prob.tspan
tdir = sign(tspan[end] - tspan[1])

t = tspan[1]
\end{lstlisting}

\texttt{sign} correctly propagates NaNs, so \texttt{tdir} now contains a NaN.

We \emph{could} trace this variable through the code, but that would be a lot of grunt work.
Instead, we can look at the NaN kill logs to get a clue as to where we should look next:

\begin{verbatim}
…
< at FloatTracker/src/TrackedFloat.jl:193
solve! at OrdinaryDiffEq/src/solve.jl:515
…
\end{verbatim}

We get a very large kill file from this run, and the two lines from above show up repeatedly.
Thus, this is a good candidate location for where the cause of the loop is.

The relevant part of \texttt{solve.jl} looks like this:

% Note for Ashton:
% file here: ~/Research/ode_debug/dev/OrdinaryDiffEq/src/solve.jl
% see line 514

\begin{lstlisting}[language = Julia]
while !isempty(time_stops)
  while tdir * t < first(time_stops)
    # do integration work
  end
  pop_if_work_done(time_stops)
end
\end{lstlisting}

The problem here is on line 2 when \texttt{tdir} is NaN: multiplication propagates NaNs, and comparison kills it, so the condition on the inner \texttt{while} loop is \emph{always} false.
No work got done in the inner loop, and so the conditional \texttt{pop} routine never reduced the size of the \texttt{time\_stops} vector.

Our kill logs led us right to this line; this is a perfect example of how NaN kills can influence control flow to go awry.
In this case, the problem was apparent besides what our logs told us and it manifested as an infinite loop.
More dangerous cases can occur when the influence of a bad comparison is not as readily observable.

\subsection{NaN Kills: Finch}

% Advection examples go here, right?

\section{Evaluation}

\section{Discussion}

\section{Related Work}

\section{Acknowledgments}

\input{bib.tex}

\end{document}

% Below is some Emacs-specific stuff. If you're using Emacs, you should try
% using the new jinx package for spell-checking!

% Local Variables:
% jinx-local-words: "CSTGs JuliaHub OrdinaryDiffEq ShallowWaters TrackedFloat"
% End:
