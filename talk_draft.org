#+title: JuliaCon 2023 Talk Draft
#+startup: showall

Time slot (according to the calendar invite) is half an hour.

Sketch: 2x 10-minute halves—first half is about how scientists would use this tool, and the second half is how we use Julia to do this in a convenient way.

Don't forget to mention GPU FPX at some point!

So:

 - scientist story
 - case studies
 - how we did this in Julia

*Notes on the slides:* Julia has it's own font [[https://juliamono.netlify.app/][JuliaMono]] and it will look extra-classy if we use that in all the examples. It's not my favorite font for code, but it's not bad either. Good character coverage.

* Talk

** Introduction: the busy scientist

Suppose that you are a scientist doing some numerical work. Maybe you're crunching data, maybe you're running a simulation, maybe you're training a model—whatever. You've got your workload spread across some CPUs and GPUs.

All is going well, until somewhere, somehow, a NaN creeps in and starts rendering the simulation unstable. Poof—all the time you spent setting up and running this simulation has been wasted, and now you're looking at hours of manually tracing through the code, rerunning your simulation and praying that you're logging the right things in the right places at the right times to catch the NaN.

Sounds like fun, no?

After hours and hours of tedious, boring, grunt work, you finally figure out that a NaN is coming from a division where the dividend underflowed to 0. You fine-tune some of the arithmetic to avoid this problem and, eyes brimming full of hope and excitement, you re-run your program with the final fix in place.

And it… works. Sort of. Something seems off about the results that you've gotten. Maybe the numbers don't look quite right. Maybe the program hangs for no apparent reason when one of the inputs changes—but you don't know what inputs matter for that. You're uncertain and uneasy about the results you're getting.

Not wanting to fall into the trap of, "the computer did it so it must be correct", you—intrepid scientist you—embark once again into the bowels of your arithmetic routines.

After a week of toil, you notice something strange: a NaN pops into existence at one point, but then quietly disappears leaving no hole in the output. After some searching, you uncover the killer: a conditional has swallowed the NaN whole.

# Slide: 42 < NaN ⇒ false
#        42 > NaN ⇒ false

See the problem?

You add a guard here for safety, then trace /that/ NaN back to Inf - Inf, which of course leads to the question: where did that Inf come from?

Another week of painful debugging goes by until you /finally/ figure out where values got too big too fast, or where one of the inputs in the 10 GB input file was missing a decimal point, etc.

This time when you run the program, the output matches the your intuition.

Hooray! Time to deploy this code to production!

*TODO: show the short clip of the car running into the wall here* https://www.youtube.com/watch?v=x4fdUx6d4QM

Well… not so fast.

# https://www.thedrive.com/news/37366/why-that-autonomous-race-car-crashed-straight-into-a-wall
You remember this mishap with the autonomous vehicle. See, what happened was that a faulty sensor returned a NaN, locking the steering column to the right.

You begin to wonder if there are any other parts of their code that could be susceptible to such problematic behavior. It might be nice if you could somehow fuzz the code to find =NaN=-susceptible regions. But trying to figure /that/ out would take a /ton/ of effort, right?

Fortunately, we have tools to help!

Floating-point numbers underpin so many critical computations. The IEEE 754 spec is a useful and performant way of representing numbers, but there are many counterintuitive aspects of both floating-point's intrinsic behavior as well as of the spec that can invalidate our results—or worse—silently cause errors. Moreover, it can be difficult to harden our programs against exceptional values. Unexpected floating-point behavior has lead not only to race car crashes but also to rockets exploding or medical patients getting fatal doses of radiation.

So, just remember: if there's somethin' strange, in your floating-points, who you gonna call? *FloatTracker!* If there's somethin' weird and it don't look good, who you gonna call? *FloatTracker!*

If, instead of doing all that manual work, you simply used our FloatTracker tool, you would have had logs that would have lead you immediately to where the NaNs and Infs were being generated and where they were disappearing, as well as tools to help you fuzz your code and find cases where you could harden your routines against spurious NaNs that didn't show up in testing.

# Sign posting!
We'll now take a brief foray into where floating-point is liable to trip you up. After that we'll talk about how to use our tool to make debugging your numerical programs a breeze. Then we'll wrap up with a quick look at some of the neat aspects of Julia that made building such a tool possible for our small research team.

# timing: ≈ 4 minutes here, second draft

** The dark world of floating-point arithmetic

Let's take a quick look at why floating-point can be so tricky.

Consider this loop:

#+begin_src julia :results output
  i::Float32 = 0.0
  n::Int = 0
  while i < 1.0
    global i += 0.1
    global n += 1
  end

  println("Did $n iterations with a Float32")

  j::Float64 = 0.0
  m::Int = 0
  while j < 1.0
    global j += 0.1
    global m += 1
  end

  println("Did $m iterations with a Float64")
#+end_src

#+RESULTS:
: 0.0
: 0
: Did 10 iterations with a Float32
: 0.0
: 0
: Did 11 iterations with a Float64

Did you see that? A 64-bit float gives you /11/ iterations instead of 10 like we'd expect. Why does this happen? Well, it all goes back to how floating-point is an /approximation/ of real numbers. We only have 16-, 32-, or 64-bits to work with, so instead of a smooth number line, we actually have a series of /points/ that we can move between.

# Show graph like from [cite:@torontoPracticallyAccurateFloatingPoint2014]

There's necessarily some gap between the values that we are trying to represent and the values that we /can/ represent. This means that there is always some kind of error. Moreover, that error accumulates throughout a computation. There are ways to work around this error, and for simple calculations it's not that important, but sometimes it can push us just over the brink into /exceptional values/.

*** Exceptional values

There are two main exceptional values that you've likely run into: =Inf= and =NaN=. =Inf= of course represents a value too /large/ to fit into your representation, be that a 16-, 32-, or 64-bit float. Once a value goes to =Inf=, there's no coming back.

In this example, we take /algebraically equivalent/ expressions and get different answers.

#+begin_src julia
  x::Float32 = 2f38
  y::Float32 = 1f38
  [(x + x) - y, x + (x - y)]
#+end_src

#+RESULTS:
|   Inf |
| 3e+38 |

This means that addition is /not associative/! We are not working with real numbers here, people! If the first example with the loops didn't scare you, this one should scare you good and proper.

=Inf= often begets =NaN= (though that's not the only place where it can come from) which denotes some nonsensical computation.

#+begin_src julia
  Inf - Inf
#+end_src

#+RESULTS:
: NaN

Sometimes it also arises from bad sensor data, typos in data, etc. That race car's issue came from a NaN on a sensor bus.

=NaN= is a /sticky/ value: almost all operations with =NaN= result in a =NaN=. This is good because if a =NaN= crops up in our computation, we want to /see/ it in the result.

For example, in this simulation, somewhere a value goes to =NaN= and it starts breaking apart the simulation as subsequent time steps take into account neighboring data points.

*TODO: the animations from Eric's talk here would be fantastic*

Now, I said that /almost all/ operations involving =NaN= can result in a =NaN=. There are cases where the NaN can disappear silently—we call this a "kill". These can be especially dangerous as it can result in the *wrong* value for a computation without us ever knowing.

Here is a short example from our paper, which you can also find in our examples repository. This is a problematic implementation of a ~list_max~ function:

# TODO: see if I can't clean up this example a little bit

#+begin_src julia
  function maximum(lst)
    max_seen = 0.0
    for x in lst
      if ! (x <= max_seen)
        max_seen = x              # swap if new val greater
      end
    end
    max_seen
  end

  maximum([1, 5, 4, NaN, 4])
#+end_src

#+RESULTS:
: 4.0

Not only does this fail to propagate the =NaN=, but the answer is /wrong/! =NaN= kills are very dangerous.

** Introducing FloatTracker

Our toolkit includes a library called FloatTracker. FloatTracker automatically notices when a =NaN= or an =Inf= gets generated or killed.

FloatTracker is easy to use. Suppose you have some simulation that looks roughly like this:

#+begin_src julia
  using MySimulator

  P = run_model(cfl=0.8, iterations=100,
                param1="nonperiodic", param2="double_gyre",
                param3="seamount")

  savefig("output.fig")
#+end_src

All it takes to add FloatTracker is:

 1. Require the library
 2. Set up the logger
 3. Wrap inputs in ~TrackedFloat~ types
 4. Flush logs

#+begin_src julia
  using MySimulator
  using FloatTracker

  P = run_model(cfl=TrackedFloat32(0.8), iterations=100,
                param1="nonperiodic", param2="double_gyre",
                param3="seamount")

  savefig("output.fig")
  ft_flush_logs()
#+end_src

It's as simple as that. FloatTracker will log every operation that the ~TrackedFloat~ type touches. Also, like a virus, the ~TrackedFloat~ type will stick to anything it comes into contact with, which means getting good coverage is relatively easy. We are working on some ways to make tracking input data even easier—that said, we've had good success with this approach so far.

Some libraries, like Finch or ShallowWaters—both of which we'll see in some of our case studies—let you /parameterize/ the type of float to use. This seems to be not uncommon with Julia libraries. If you are fortunate enough to be using such a library, simply use a ~TrackedFloat~.

** Case studies

We took our tool out for a spin on some Julia libraries. The first library we'll look at is =ShallowWaters.jl= —a program for doing shallow ocean simulation.

*** ShallowWaters

ShallowWaters lets you take a mesh of a sea bed and then run a time series simulation and get the speed and direction of currents over that sea floor.

# TODO: show example pictures, as well as the code to start a run.

**** NaNs from instability

I'd like to highlight this parameter here: the =CFL= parameter. I'm no domain expert, but this value control the time step rate in the simulation. A lower value means a more granular, careful flow of time in the simulation, but it means the simulation doesn't complete as quickly.

In contrast, a higher =CFL= parameter means the simulation goes faster, but it can result in instability. If we set the =CFL= parameter /really/ high, we start seeing some instability in the simulation.

# TODO: show NaN-broken simulation pictures

I'll note that while in our case the =CFL= parameter is a little unrealistic, it's not uncommon to have an issue with simulation instability. Remember this graph from earlier?

*TODO: show Eric H.'s graph again*

This is a similar sort of issue.

Figuring out /where/ the NaN was coming from would be difficult to do manually, but we can add FloatTracker to the code like this:

# TODO: show little animation/diff on adding FloatTracker to ShallowWaters

We make just two simple changes:

 - we set up some logging for FloatTracker [highlight on slide]
 - we wrap the input in ~TrackedFloat~

With that we get some nice logs about where those NaNs are coming from.

To get a quick summary, we can /coalesce/ the logs into a handy graph that lets us see where most fo the flows are going to/or from.

# FIXME: figure out what the exact operation here is
We can see on this line in this file [highlight] that a NaN appears when we do [ *FIXME: show operation* ].

Now we leave it to a domain expert to figure out how to mitigate this. Some strategies:

 - use a bigger bit-width
 - use a tool like Herbie to rewrite floating-point expressions to reduce error
 - manual reorder operations to keep values from getting too big

*** Fuzzing: OrdinaryDiffEq

Next we took a look at the =OrdinaryDiffEq= library—a commonly used library for… you guessed it… differential equations.

Since this is such a highly used library, it's important to ensure that there are no =NaN= kills in this library.

FloatTracker has a utility akin to fuzz testing that lets us randomly /inject/ =NaNs= into the computation. We can then watch the logs for any =NaN= kills and make corrections.

Without too much effort, we were able to find one place where a =NaN= would cause =OrdinaryDiffEq= to go into an infinite loop because of a =NaN= kill. It wasn't a common case, but it was a behavioral issue that we were quickly able to identify and rectify with FloatTracker.

*** RxInfer

FloatTracker isn't just for "PL wizards"—this is a /useful/ tool that others have tried out and gotten good results with. We came across an issue with the =RxInfer= package, a library for Bayesian inference. We found an issue that said:

#+begin_quote
Now it is impossible to trace back the origin of the very first ~NaN~ without perform a lot of manual work. This limits the ability to debug the code and to prevent these ~NaN~s in the first place.

RxInfer.jl#116
#+end_quote

They were doing work with some proprietary information, so we were not able to help them out ourselves. However, they /did/ try our tool out. In less than a day, they got our tool up and running and found the issue.

** How we made this work

Now we'll talk a little bit about how we got FloatTracker to work. In principle we're not doing anything that couldn't be done in another language, but Julia makes it /really/ easy to create the kind of tool that we did.

Julia, as you're aware, uses a /type-based dispatch mechanism/, and it's JIT compiler is tuned to optimize these sorts of calls. Moreover, the standard library is just made of functions, and we can define our own kinds.

For example, there are over 200 definitions for ~+~ out-of-the-box. Julia efficiently handles dispatching to the right value depending on what appears at runtime.

With FloatTracker:

 - We take the built-in ~Float16~, ~Float32~, and ~Float64~ and we replace them with our own types ~TrackedFloat16~, ~TrackedFloat32~, and ~TrackedFloat64~.
 - Once that is done, we overload all the built-in operators and functions to intercept function calls and do the right thing

*** Some more details

We start by defining a new data type that wraps a regular float:

#+begin_src julia
  abstract type AbstractTrackedFloat <: AbstractFloat end

  struct TrackedFloat32 <: AbstractTrackedFloat
    val::Float32
  end
#+end_src

And then all we have to do is implement overloaded methods for this type:

#+begin_src julia
  function Base.+(x::TrackedFloat32, y::TrackedFloat32)
    result = x.val + y.val
    check_error(+, result, x.val, y.val)
    TrackedFloat32(r)
  end
#+end_src

# TODO: walk through this carefully
# Talk about:
#  - running the function
#  - (maybe mention how this is where we can also inject NaNs for fuzzing?)
#  - check for interesting exceptional value events
#  - return a new wrapped value

*** Using meta programming

That, as you might assume, would be tedious to write out for every function, not to mention impossible to maintain. Fortunately, Julia lets us use /macros/, so we can automate an impressive amount of things.

You can write two nested ~for~ loops to quickly generate the code needed for this:

#+begin_src julia
  for TrackedFloatN in (:TrackedFloat16, :TrackedFloat32, :TrackedFloat64)
    for Op in (:+, :-, :/, :^)
      @eval function Base.$Op(x::$TrackedFloatN, y::$TrackedFloatN)
        result = $Op(x, y)
        check_error($Op, result, x.val, y.val)
        $TrackedFloatN(r)
      end
    end
  end
#+end_src

# TODO: add some nice slides walking through the different parts of that example

It's a little more complicated than that to handle a few edge cases, but /not by much/.

We generate
 - 3 structs
 - 645 function variants
 - only 218 lines of code, about 23 of which are devoted to defining helper functions and boilerplate

** GPU utilities

# TODO

** Conclusion

Despite it's young age, FloatTracker has been useful not only to /us/ as researchers, but also to developers like you diagnose floating-point exceptions. It can be a valuable tool for hardening floating-point code against inadvertent =NaN= kills which can lead to baffling behavior or silently incorrect results.

We've been able to exercise some exciting metaprogramming abilities of Julia to make this possible.

Thank you for your attention. We hope you find FloatTracker useful to you as you write numerical code. I'll be happy to answer your questions now.
